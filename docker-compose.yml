version: '3'
services:
  #
  ## SERVICE1: APACHE SPARK -------------------------------------
  #  spark:
  #    image: docker.io/bitnami/spark:3
  #    environment:
  #      - SPARK_MODE=master
  #      - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #      - SPARK_RPC_ENCRYPTION_ENABLED=no
  #      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #      - SPARK_SSL_ENABLED=no
  #    ports:
  #      - '8080:8080'
  #  spark-worker:
  #    image: docker.io/bitnami/spark:3
  #    environment:
  #      - SPARK_MODE=worker
  #      - SPARK_MASTER_URL=spark://spark:7077
  #      - SPARK_WORKER_MEMORY=1G
  #      - SPARK_WORKER_CORES=1
  #      - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #      - SPARK_RPC_ENCRYPTION_ENABLED=no
  #      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #      - SPARK_SSL_ENABLED=no
  #

  # SERVICE2: HDFS ---------------------------------------------
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name # volume di hdfs namenode
      - nifi-hdfs:/etc/hadoop # volume in comune con nifi
      - type: volume
        source: hdfs-volume
        target: /etc/hadoop/
        volume:
          - nocopy: true
      - type: bind
        source: nifi-hdfs/core-site.xml
        target: /etc/hadoop/core-site.xml
      - type: bind
        source: nifi-hdfs/hdfs-site.xml
        target: /etc/hadoop/hdfs-site.xml

    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env

  # SERVICE3: NIFI ----------------------------------------------
  nifi:
    image: apache/nifi:latest
    restart: always
    container_name: nifi
    ports:
      - '8181:8181'
      # - '9000:9000' # serve per connettersi al namenode hdfs
    environment:
      NIFI_WEB_HTTP_PORT: '8181'
    volumes:
      - .nifi:/opt/nifi/nifi-current/ls-target
      - type: bind
        source: .nifi/security/cacerts
        target: /opt/nifi/nifi-current/cacerts
      - nifi-hdfs:/opt/nifi/nifi-current/ls-target/hdfs-conf # montiamo il volume nifi-hdfs nella cartella hdfs-conf, che è collegata anche in .nifi/hdfs-conf

# nifi salva su HDFS, per vedere il file usa: "hdfs dfs -ls /home/dataset-batch"


# SERVICE4: REDIS ---------------------------------------------
#  redis:
#    image: redis
#    container_name: redis-cache
#    ports:
#      - #to be defined

volumes:
  nifi-hdfs: # volume in comune tra hdfs
    driver: local # Dove salvare i dati? Di default è local
    external: false # Il container è stato definito fuori da docker-compose? Di default è false.
  hadoop_datanode:
    driver: local # Dove salvare i dati? Di default è local
    external: false # Il container è stato definito fuori da docker-compose? Di default è false.
  hadoop_namenode:
    driver: local # Dove salvare i dati? Di default è local
    external: false # Il container è stato definito fuori da docker-compose? Di default è false.
