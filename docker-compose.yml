version: '3'
services:

  ## SERVICE3: NIFI ----------------------------------------------
  nifi:
    image: apache/nifi:latest
    restart: always
    container_name: nifi
    ports:
      - '8181:8181'   # http://localhost:8181 web ui NiFi
    environment:
      NIFI_WEB_HTTP_PORT: '8181'
    volumes:
      - .nifi:/opt/nifi/nifi-current/ls-target
      - type: bind
        source: .nifi/security/cacerts
        target: /opt/nifi/nifi-current/cacerts
      - nifi-hdfs:/opt/nifi/nifi-current/ls-target/hdfs-conf # loads configuration files from hdfs to nifi, even if are not copied in the local fs

  # nifi salva su HDFS, per vedere il file usa: "hdfs dfs -ls /home/dataset-batch"

  ## SERVICE1: APACHE SPARK -------------------------------------
  spark:
    image: docker.io/bitnami/spark:3
    container_name: spark-master
    restart: always
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - '8080:8080' # http://localhost:8080 web ui spark
      - '7077:7077' # per connettersi a spark master
      - '4040:4040'
    volumes:
      - ./target:/opt/bitnami/spark/taxi-app/
      - .nifi/templates:/home/templates
      - ./Results:/home/results

  spark-worker:
    image: docker.io/bitnami/spark:3
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077 # the url of the master the worker will connect to
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    #    ports:
    #      - '8081:8081' # http://localhost:8081 web ui spark
    volumes:
      - ./target:/opt/bitnami/spark/taxi-app/
    depends_on:
      - spark

  # SERVICE2: HDFS ---------------------------------------------
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - '9870:9870' # http://localhost:9870 web ui HDFS.
      - '9000:9000' # HDFS master
    volumes:
      - hadoop_namenode:/hadoop/dfs/name # volume di hdfs namenode
      - nifi-hdfs:/etc/hadoop # shared with nifi
    environment:
      - CLUSTER_NAME=test
      - HDFS_CONF_dfs_replication=1
    env_file:
      - ./hadoop.env

  datanode: # se il datanode va in un ciclo di riavvii, bisogna eliminare tutti i container e volumi di questo compose e riavviare.
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
#    environment:
#      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
    depends_on:
      - namenode


# SERVICE4: REDIS ---------------------------------------------
  redis:
    image: redis
    container_name: redis-cache
    ports:
      - '6379:6379'

# SERVICE5: GRAFANA -------------------------------------------
  grafana:
    build:
      context: ./grafana
    container_name: grafana
    ports:
      - '8000:3000'
    volumes:
      - grafana-data:/var/lib/grafana

volumes:
  nifi-hdfs: # volume shared by nifi and hdfs
  nifi-spark: #volume shared by nifi and spark
  hadoop_datanode:
    driver: local # where to save data. Default is local
    external: false # volume defined outside this docker-compose? Default is false.
  hadoop_namenode:
    driver: local
    external: false
  grafana-data:
    external: false
